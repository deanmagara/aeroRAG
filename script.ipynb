{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a65b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0. Setup and Imports ---\n",
    "print(\"--- 0. Setting up Colab environment and imports ---\")\n",
    "# Install the necessary text-splitter library (Colab specific)\n",
    "!pip install --quiet langchain-text-splitters\n",
    "import json\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import requests\n",
    "import io\n",
    "import time\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Configuration ---\n",
    "URL = \"https://ntrs.staging.sti.appdat.jsc.nasa.gov/api/docs/ntrs-public-metadata.json.gz?attachment=true\"\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# --- Helper Function: Load Data (Download, Decompress, Load) ---\n",
    "def load_data_from_url(url):\n",
    "    \"\"\"Downloads, decompresses, and loads the single JSON object into a DataFrame.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n1. Starting Data Acquisition and Loading from URL...\")\n",
    "    try:\n",
    "        # 1. Download the file content\n",
    "        print(\"   📥 Downloading compressed file...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # 2. Decompress\n",
    "        print(\"   📦 Decompressing gzip file...\")\n",
    "        compressed_file = io.BytesIO(response.content)\n",
    "\n",
    "        # Use gzip to open the compressed content as text\n",
    "        with gzip.open(compressed_file, 'rt', encoding='utf-8') as f:\n",
    "            # 3. Load the single JSON object\n",
    "            print(\"   📖 Parsing JSON data...\")\n",
    "            data = json.load(f)\n",
    "\n",
    "        print(f\"   ✅ Data loaded. Type: {type(data)}. Size: {len(data):,} items.\")\n",
    "\n",
    "        # 4. Convert to DataFrame\n",
    "        records = []\n",
    "        if isinstance(data, dict):\n",
    "            for doc_id, doc_data in data.items():\n",
    "                if isinstance(doc_data, dict):\n",
    "                    record = doc_data.copy()\n",
    "                    record['document_id'] = doc_id\n",
    "                    records.append(record)\n",
    "\n",
    "            df = pd.DataFrame(records)\n",
    "            print(f\"   ✅ Converted to DataFrame with {len(df):,} rows.\")\n",
    "            return df\n",
    "        else:\n",
    "            print(\"   ❌ Error: Data root structure is not a dictionary.\")\n",
    "            return None\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"   ❌ ERROR: Network/Download error: {e}\")\n",
    "        return None\n",
    "    except (gzip.BadGzipFile, json.JSONDecodeError) as e:\n",
    "        print(f\"   ❌ ERROR: Decompression or JSON parsing error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        print(f\"   (Data Acquisition took {time.time() - start_time:.2f} seconds)\")\n",
    "\n",
    "\n",
    "# --- Step 1: Data Cleaning and Preprocessing ---\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Cleans, preprocesses, and validates the data.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"\\n2. Starting Data Cleaning and Preprocessing...\")\n",
    "\n",
    "    # --- Data Cleaning and Validation ---\n",
    "    df['abstract'] = df['abstract'].fillna(\"\").astype(str)\n",
    "    df['keywords'] = df['keywords'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    initial_count = len(df)\n",
    "    df = df[df['title'].astype(str).str.strip().str.len() > 0]\n",
    "    print(f\"   - Validation: Filtered {initial_count - len(df)} records with missing titles.\")\n",
    "\n",
    "    # --- Data Feature Engineering (Flattening) ---\n",
    "    def flatten_authors(affiliations):\n",
    "        if not isinstance(affiliations, list): return \"\"\n",
    "        author_info = []\n",
    "        for item in affiliations:\n",
    "            try:\n",
    "                if 'meta' in item and 'author' in item['meta']:\n",
    "                    name = item['meta']['author'].get('name', '')\n",
    "                    if name: author_info.append(name)\n",
    "            except:\n",
    "                continue\n",
    "        return \", \".join(sorted(list(set(author_info))))\n",
    "\n",
    "    def list_to_string(item_list):\n",
    "        if not isinstance(item_list, list): return \"\"\n",
    "        return \" | \".join(str(item) for item in item_list)\n",
    "\n",
    "    df['authors_flat'] = df['authorAffiliations'].apply(flatten_authors)\n",
    "    df['keywords_flat'] = df['keywords'].apply(list_to_string)\n",
    "\n",
    "    # --- Core Text Generation (The RAG Source) ---\n",
    "    df['text_source'] = (\n",
    "        \"TITLE: \" + df['title'].astype(str) +\n",
    "        \"\\nABSTRACT: \" + df['abstract'].astype(str) +\n",
    "        \"\\nAUTHORS: \" + df['authors_flat'].astype(str) +\n",
    "        \"\\nKEYWORDS: \" + df['keywords_flat'].astype(str)\n",
    "    )\n",
    "\n",
    "    df_rag = df[['document_id', 'title', 'abstract', 'text_source']].copy()\n",
    "\n",
    "    print(f\"   ✅ Preprocessing complete. Final record count: {len(df_rag):,}.\")\n",
    "    print(f\"   (Preprocessing took {time.time() - start_time:.2f} seconds)\")\n",
    "    return df_rag\n",
    "\n",
    "\n",
    "# --- Step 2: Chunking Strategy Implementation ---\n",
    "def chunk_data(df_rag):\n",
    "    \"\"\"Implements intelligent chunking.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(\"\\n3. Starting Intelligent Chunking...\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    total_docs = len(df_rag)\n",
    "\n",
    "    for index, row in df_rag.iterrows():\n",
    "        doc_text = row['text_source']\n",
    "        doc_id = row['document_id']\n",
    "        doc_title = row['title']\n",
    "\n",
    "        text_chunks = text_splitter.split_text(doc_text)\n",
    "\n",
    "        for i, chunk_text in enumerate(text_chunks):\n",
    "            chunks.append({\n",
    "                'document_id': doc_id,\n",
    "                'title': doc_title,\n",
    "                'chunk_id': f\"{doc_id}-{i+1}\",\n",
    "                'chunk_text': chunk_text,\n",
    "                'chunk_size': len(chunk_text)\n",
    "            })\n",
    "\n",
    "    df_chunks = pd.DataFrame(chunks)\n",
    "\n",
    "    print(f\"   ✅ Chunking complete. Total documents processed: {total_docs:,}\")\n",
    "    print(f\"   Total chunks created: {len(df_chunks):,}\")\n",
    "    print(f\"   (Chunking took {time.time() - start_time:.2f} seconds)\")\n",
    "    return df_chunks\n",
    "\n",
    "\n",
    "# --- Step 3: Pipeline Documentation and Execution ---\n",
    "def run_pipeline():\n",
    "    \"\"\"Executes the full pipeline and provides documentation.\"\"\"\n",
    "\n",
    "    pipeline_doc = f\"\"\"\n",
    "============================================================\n",
    "PIPELINE DOCUMENTATION: NASA NTRS METADATA RAG PREPARATION\n",
    "============================================================\n",
    "\n",
    "1. DATA ACQUISITION & LOADING:\n",
    "- Method: Direct download via Python 'requests' and in-memory decompression with 'gzip'.\n",
    "- Source: {URL}\n",
    "\n",
    "2. DATA CLEANING, PREPROCESSING & VALIDATION:\n",
    "- **Core Text Generation**: Master field (`text_source`) created from TITLE, ABSTRACT, AUTHORS, and KEYWORDS.\n",
    "- **Validation**: Records with missing titles are dropped.\n",
    "\n",
    "3. CHUNKING STRATEGY:\n",
    "- **Tool**: RecursiveCharacterTextSplitter.\n",
    "- **Goal**: Maintain semantic boundaries.\n",
    "- **Parameters**: Chunk Size: {CHUNK_SIZE}, Chunk Overlap: {CHUNK_OVERLAP}.\n",
    "\n",
    "4. FINAL OUTPUT:\n",
    "- DataFrame `df_chunks` ready for vector embedding.\n",
    "============================================================\n",
    "\"\"\"\n",
    "    print(pipeline_doc)\n",
    "\n",
    "    # Execution\n",
    "    df_raw = load_data_from_url(URL)\n",
    "    if df_raw is None:\n",
    "        return None, None\n",
    "\n",
    "    df_rag = preprocess_data(df_raw)\n",
    "\n",
    "    df_chunks = chunk_data(df_rag)\n",
    "\n",
    "    print(f\"\\n--- Final Output: df_chunks Head (First Chunk) ---\")\n",
    "    print(df_chunks.head(1).T)\n",
    "    print(f\"\\nPipeline Execution Complete! Total final chunks: {len(df_chunks):,}\")\n",
    "\n",
    "    # *** KEY CHANGE: Return the DataFrames ***\n",
    "    return df_rag, df_chunks\n",
    "\n",
    "# This execution block needs to be run in the *same cell* as the functions.\n",
    "# It assigns the returned values to global variables.\n",
    "# You can delete the original 'if __name__ == \"__main__\":' block if it was in a separate cell.\n",
    "\n",
    "# *** Assign the returned DataFrames to global variables ***\n",
    "global df_rag, df_chunks\n",
    "df_rag, df_chunks = run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0301258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Full Combined Text Source for the First Document (df_rag) ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_rag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Full Combined Text Source for the First Document (df_rag) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m doc_index = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDocument ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_rag\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mdocument_id\u001b[39m\u001b[33m'\u001b[39m].iloc[doc_index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_rag[\u001b[33m'\u001b[39m\u001b[33mtext_source\u001b[39m\u001b[33m'\u001b[39m].iloc[doc_index])\n",
      "\u001b[31mNameError\u001b[39m: name 'df_rag' is not defined"
     ]
    }
   ],
   "source": [
    "# View the full combined text source for the first document\n",
    "print(\"\\n--- Full Combined Text Source for the First Document (df_rag) ---\")\n",
    "doc_index = 0\n",
    "print(f\"Document ID: {df_rag['document_id'].iloc[doc_index]}\")\n",
    "print(\"---\")\n",
    "print(df_rag['text_source'].iloc[doc_index])\n",
    "\n",
    "# View the first 5 chunks to see how the text was split\n",
    "print(\"\\n--- Preview of the Final Chunks (df_chunks) ---\")\n",
    "print(f\"Total Chunks: {len(df_chunks):,}\\n\")\n",
    "print(df_chunks.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
